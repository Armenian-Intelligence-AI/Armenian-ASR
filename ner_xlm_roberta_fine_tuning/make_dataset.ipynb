{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T07:39:00.802205Z",
     "start_time": "2024-10-16T07:35:26.865657Z"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def get_ner_tag(word):\n",
    "    \"\"\"Assign NER tag based on word's capitalization.\"\"\"\n",
    "    if word.islower():\n",
    "        return 0\n",
    "    elif word.istitle():\n",
    "        return 1\n",
    "    elif word.isupper():\n",
    "        return 2\n",
    "    return 0  # default to 0 if none of the conditions match\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    \"\"\"Tokenize sentence and generate corresponding NER tags.\"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    ner_tags = [get_ner_tag(token) for token in tokens]\n",
    "    return tokens, ner_tags\n",
    "\n",
    "def create_dataset_from_txt_files(folder_path):\n",
    "    \"\"\"Process all .txt files and create dataset with id, tokens, and ner_tags.\"\"\"\n",
    "    dataset = {\"id\": [], \"tokens\": [], \"ner_tags\": []}\n",
    "\n",
    "    for idx, filename in tqdm(enumerate(os.listdir(folder_path))):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                sentences = f.readlines()\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    tokens, ner_tags = process_sentence(sentence.strip())\n",
    "                    dataset[\"id\"].append(f\"{idx}\")\n",
    "                    dataset[\"tokens\"].append(tokens)\n",
    "                    dataset[\"ner_tags\"].append(ner_tags)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.7, validation_ratio=0.15):\n",
    "    \"\"\"Split dataset into train, validation, and test sets.\"\"\"\n",
    "    num_samples = len(dataset[\"id\"])\n",
    "    train_size = int(num_samples * train_ratio)\n",
    "    validation_size = int(num_samples * validation_ratio)\n",
    "\n",
    "    train_dataset = {\n",
    "        \"id\": dataset[\"id\"][:train_size],\n",
    "        \"tokens\": dataset[\"tokens\"][:train_size],\n",
    "        \"ner_tags\": dataset[\"ner_tags\"][:train_size]\n",
    "    }\n",
    "\n",
    "    validation_dataset = {\n",
    "        \"id\": dataset[\"id\"][train_size:train_size + validation_size],\n",
    "        \"tokens\": dataset[\"tokens\"][train_size:train_size + validation_size],\n",
    "        \"ner_tags\": dataset[\"ner_tags\"][train_size:train_size + validation_size]\n",
    "    }\n",
    "\n",
    "    test_dataset = {\n",
    "        \"id\": dataset[\"id\"][train_size + validation_size:],\n",
    "        \"tokens\": dataset[\"tokens\"][train_size + validation_size:],\n",
    "        \"ner_tags\": dataset[\"ner_tags\"][train_size + validation_size:]\n",
    "    }\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "def convert_to_hf_dataset(dataset):\n",
    "    \"\"\"Convert a dictionary to Hugging Face Dataset.\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        'id': dataset['id'],\n",
    "        'tokens': dataset['tokens'],\n",
    "        'ner_tags': dataset['ner_tags']\n",
    "    })\n",
    "\n",
    "def create_dataset_dict(folder_path):\n",
    "    \"\"\"Create DatasetDict with train, validation, and test splits.\"\"\"\n",
    "    # Step 1: Process the .txt files into a dataset\n",
    "    raw_dataset = create_dataset_from_txt_files(folder_path)\n",
    "\n",
    "    # Step 2: Split the dataset\n",
    "    train_dataset, validation_dataset, test_dataset = split_dataset(raw_dataset)\n",
    "\n",
    "    # Step 3: Convert each split to Hugging Face Dataset format\n",
    "    hf_train_dataset = convert_to_hf_dataset(train_dataset)\n",
    "    hf_validation_dataset = convert_to_hf_dataset(validation_dataset)\n",
    "    hf_test_dataset = convert_to_hf_dataset(test_dataset)\n",
    "\n",
    "    # Step 4: Combine into a DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': hf_train_dataset,\n",
    "        'validation': hf_validation_dataset,\n",
    "        'test': hf_test_dataset\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "# Specify the folder path with the .txt files\n",
    "folder_path = '/home/vahan/Documents/NER_data/final_data'\n",
    "\n",
    "# Create the DatasetDict\n",
    "dataset_dict = create_dataset_dict(folder_path)\n",
    "\n",
    "# Example: Print details of the dataset\n",
    "print(dataset_dict)\n",
    "\n",
    "# Example: Save the dataset in the Hugging Face format (optional)\n",
    "dataset_dict.save_to_disk('final_dataset')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1239961it [03:27, 5967.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 867972\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 185994\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 185995\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/867972 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8051fd8771843beb82418f64cdcbc95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/185994 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31a8240f8b1d4c218b551547df420eab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/185995 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1604593c8557421a9ddc81d69c9c0ccb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ffa57030c1a60c8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
