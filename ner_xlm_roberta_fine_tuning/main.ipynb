{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:43.270001Z",
     "start_time": "2024-10-16T11:49:41.046468Z"
    }
   },
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "from scipy.stats import differential_entropy\n",
    "\n",
    "notebook_login()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a9a636561fce40e08ef74441f5264bbc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:48.700525Z",
     "start_time": "2024-10-16T11:49:48.307412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Path where the dataset was saved\n",
    "dataset_path = 'final_dataset'  # Replace with the actual path if different\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "# Example: Print details of the loaded dataset\n",
    "print(dataset)"
   ],
   "id": "817a62fbd48bd594",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 867972\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 185994\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 185995\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:51.718368Z",
     "start_time": "2024-10-16T11:49:51.317328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lowercase_tokens(example):\n",
    "    # Convert all tokens to lowercase and strip punctuation\n",
    "    example['tokens'] = [token.strip(',[]').lower() for token in example['tokens']]\n",
    "    return example\n",
    "\n",
    "# Apply to all subsets (train, validation, test)\n",
    "dataset = dataset.map(lowercase_tokens, batched=False)\n",
    "\n",
    "# Now the dataset['train'], dataset['validation'], and dataset['test'] will have cleaned lowercase tokens\n"
   ],
   "id": "f77425524fe428e2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:55.418954Z",
     "start_time": "2024-10-16T11:49:55.402217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "# arm_model = 'ai-forever/mGPT-armenian'\n",
    "batch_size = 16"
   ],
   "id": "e4598f85a8423979",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:55.455036Z",
     "start_time": "2024-10-16T11:49:55.453494Z"
    }
   },
   "cell_type": "code",
   "source": "# dataset[\"train\"].features[f\"ner_tags\"]",
   "id": "ad3ea25a0503ffc6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:55.501170Z",
     "start_time": "2024-10-16T11:49:55.499477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# label_list = dataset[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list = ['l', 'u', 'uu']"
   ],
   "id": "d1c4e292dc7c129d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:56.533539Z",
     "start_time": "2024-10-16T11:49:55.546951Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = XLMRobertaTokenizerFast.from_pretrained(model_checkpoint)",
   "id": "582b4a4de51e02af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:56.596693Z",
     "start_time": "2024-10-16T11:49:56.594923Z"
    }
   },
   "cell_type": "code",
   "source": "label_all_tokens = True",
   "id": "34b6e4d27f831025",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:49:57.513549Z",
     "start_time": "2024-10-16T11:49:57.510831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        # print(label)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ],
   "id": "9c222717446b33ae",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:05.273782Z",
     "start_time": "2024-10-16T11:49:59.294673Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)",
   "id": "b5a816ef6b249bc9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/185994 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a155efd0a3264b01b6b8d1505a8ba426"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:08.323214Z",
     "start_time": "2024-10-16T11:50:08.321390Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer",
   "id": "e65c155bdde0e78b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:09.156556Z",
     "start_time": "2024-10-16T11:50:08.670490Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3)  # Update `label_names`",
   "id": "6467780633da5fed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:10.503390Z",
     "start_time": "2024-10-16T11:50:10.500798Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "2c4b3332ea18c8e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:11.714635Z",
     "start_time": "2024-10-16T11:50:11.711582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ],
   "id": "69dd96a85f33bdec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 277455363\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:37.645347Z",
     "start_time": "2024-10-16T11:50:37.575882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ],
   "id": "4455cb9ebf5ac5d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:38.823039Z",
     "start_time": "2024-10-16T11:50:38.820813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "id": "4e6b75b1468d4bc8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:41.754998Z",
     "start_time": "2024-10-16T11:50:39.580982Z"
    }
   },
   "cell_type": "code",
   "source": "metric = load_metric(\"seqeval\")",
   "id": "b602112068a53540",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:42.352677Z",
     "start_time": "2024-10-16T11:50:42.349899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "id": "fc30066149f91d52",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T11:50:45.802175Z",
     "start_time": "2024-10-16T11:50:44.980917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "acc3c7cd274d8af4",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T02:05:35.705602Z",
     "start_time": "2024-10-16T11:51:15.689020Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "2a8ad854b06c5b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='271245' max='271245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [271245/271245 14:14:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.064859</td>\n",
       "      <td>0.912854</td>\n",
       "      <td>0.918492</td>\n",
       "      <td>0.915665</td>\n",
       "      <td>0.978428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.060763</td>\n",
       "      <td>0.929158</td>\n",
       "      <td>0.925038</td>\n",
       "      <td>0.927094</td>\n",
       "      <td>0.980237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.048300</td>\n",
       "      <td>0.059455</td>\n",
       "      <td>0.921628</td>\n",
       "      <td>0.932429</td>\n",
       "      <td>0.926997</td>\n",
       "      <td>0.981205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.062650</td>\n",
       "      <td>0.918281</td>\n",
       "      <td>0.936071</td>\n",
       "      <td>0.927091</td>\n",
       "      <td>0.981732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.066567</td>\n",
       "      <td>0.929264</td>\n",
       "      <td>0.936230</td>\n",
       "      <td>0.932734</td>\n",
       "      <td>0.981855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: u seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: l seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: uu seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: u seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: l seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: uu seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: u seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: l seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: uu seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: u seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: l seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: uu seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: u seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: l seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/home/vahan/anaconda3/envs/NER/lib/python3.8/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: uu seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=271245, training_loss=0.05374796981866372, metrics={'train_runtime': 51258.5404, 'train_samples_per_second': 84.666, 'train_steps_per_second': 5.292, 'total_flos': 1.8127161969483776e+17, 'train_loss': 0.05374796981866372, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T11:37:30.328255950Z",
     "start_time": "2024-09-24T10:24:58.775252Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.evaluate()",
   "id": "b8133a7ad61695f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1219' max='1219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1219/1219 00:24]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0003890861407853663,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 1.0,\n",
       " 'eval_f1': 1.0,\n",
       " 'eval_accuracy': 0.9999527005720978,\n",
       " 'eval_runtime': 30.3931,\n",
       " 'eval_samples_per_second': 641.298,\n",
       " 'eval_steps_per_second': 40.108,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:50:17.607154Z",
     "start_time": "2024-10-17T11:50:07.523859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = \"./milion_ner_model_3_epoch\"  # Directory to save the model\n",
    "trainer.save_model(output_dir)  # Save model checkpoint\n",
    "tokenizer.save_pretrained(output_dir)  # Save the tokenizer"
   ],
   "id": "f044dbe711c3b4d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./milion_ner_model_3_epoch/tokenizer_config.json',\n",
       " './milion_ner_model_3_epoch/special_tokens_map.json',\n",
       " './milion_ner_model_3_epoch/sentencepiece.bpe.model',\n",
       " './milion_ner_model_3_epoch/added_tokens.json',\n",
       " './milion_ner_model_3_epoch/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:47:46.789071Z",
     "start_time": "2024-10-22T05:47:46.787132Z"
    }
   },
   "cell_type": "code",
   "source": "output_dir = \"/home/vahan/Documents/NER/milion_ner_model_3_epoch\"",
   "id": "c7a46c189d347c1f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:47:46.927320Z",
     "start_time": "2024-10-22T05:47:46.925432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "phrases_with_i = [\n",
    "    \"ի վեր\", \"ի վերջո\", \"ի նպաստ\", \"ի հեճուկս\", \"ի դեպ\"\n",
    "                                                \"ի նշան\", \"ի պատիվ\", \"ի դեմ\", \"ի պաշտպանություն\",\n",
    "    \"ի պահպանություն\", \"ի միջի\", \"ի հիշատակ\", \"ի ցույց\", \"ի գործ\"\n",
    "]\n"
   ],
   "id": "38e132422b1e8ca9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:47:47.054036Z",
     "start_time": "2024-10-22T05:47:47.052387Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import AutoTokenizer, AutoModelForTokenClassification",
   "id": "c9be6cda62b811c4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T05:47:47.643636Z",
     "start_time": "2024-10-22T05:47:47.172316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(output_dir)\n"
   ],
   "id": "a82eabd10a83e53d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T06:59:57.940532Z",
     "start_time": "2024-10-22T06:59:57.605899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import re\n",
    "from text_converter import ArmenianTextToNumberConverter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "model = AutoModelForTokenClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Function to replace 'եւ' with 'և'\n",
    "def replace_and(sentence):\n",
    "    return sentence.replace('եւ', 'և')\n",
    "\n",
    "# Function to ensure the sentence ends with a colon\n",
    "def ensure_colon(sentence):\n",
    "    return sentence if sentence.endswith('։') else sentence + '։'\n",
    "\n",
    "# Function to make the letter after a colon uppercase\n",
    "def uppercase_after_colon(sentence):\n",
    "    # Use regular expression to find a colon followed by a space and a letter, and uppercase that letter\n",
    "    return re.sub(r'(։\\s*)(\\w)', lambda match: match.group(1) + match.group(2).upper(), sentence)\n",
    "\n",
    "# Function to correct sentence based on token classification model predictions\n",
    "def correct_sentence(input_sentence, tokenizer, model):\n",
    "    # Tokenize input sentence\n",
    "    tokenized_input = tokenizer(\n",
    "        input_sentence.split(),\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokenized_input)\n",
    "\n",
    "    # Extract predicted token IDs and convert back to tokens\n",
    "    predicted_ids = output.logits.argmax(dim=2)[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'][0])\n",
    "\n",
    "    # Define label names and special tokens\n",
    "    label_names = ['O', '1', '2']\n",
    "    special_tokens = set(tokenizer.all_special_tokens)\n",
    "\n",
    "    corrected_sentence = []\n",
    "    current_word = \"\"\n",
    "\n",
    "    # Process each token and its predicted label\n",
    "    for token, predicted_id in zip(tokens, predicted_ids):\n",
    "        label = label_names[predicted_id]\n",
    "\n",
    "        if token in special_tokens:\n",
    "            continue\n",
    "\n",
    "        # Handle word continuation or start\n",
    "        if token.startswith(\"▁\"):\n",
    "            if current_word:\n",
    "                corrected_sentence.append(current_word)\n",
    "            current_word = token[1:]\n",
    "        else:\n",
    "            current_word += token\n",
    "\n",
    "        # Apply corrections based on label\n",
    "        if label == '2':\n",
    "            current_word = current_word.upper()\n",
    "        elif label != 'O':\n",
    "            current_word = current_word.capitalize()\n",
    "\n",
    "    if current_word:\n",
    "        corrected_sentence.append(current_word)\n",
    "\n",
    "    # Join corrected words to form the final sentence\n",
    "    final_sentence = \" \".join(corrected_sentence)\n",
    "\n",
    "    # Uppercase the letter after a colon\n",
    "    final_sentence = uppercase_after_colon(final_sentence)\n",
    "\n",
    "    return final_sentence\n",
    "\n",
    "\n",
    "# Define allowed punctuations\n",
    "allowed_punctuations = {',', '։', '՝', '՞', '-', '.'}\n",
    "\n",
    "\n",
    "# Function to add spaces around punctuations\n",
    "def add_space_between_punctuation(sentence, punctuations):\n",
    "    # Regular expression for allowed punctuations\n",
    "    pattern = f\"([{''.join(re.escape(p) for p in punctuations)}])\"\n",
    "\n",
    "    # Add space around hyphen\n",
    "    corrected_sentence = re.sub(r\"(\\w)-(\\w)\", r\"\\1 - \\2\", sentence)\n",
    "\n",
    "    # Add space before and after other punctuations\n",
    "    corrected_sentence = re.sub(rf\"(\\S)({pattern})\", r\"\\1 \\2\", corrected_sentence)\n",
    "    corrected_sentence = re.sub(rf\"({pattern})(\\S)\", r\"\\1 \\2\", corrected_sentence)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "\n",
    "# Function to merge spaces around punctuations back to original form\n",
    "def merge_same_punctuation(sentence, punctuations):\n",
    "    # Regular expression for allowed punctuations\n",
    "    pattern = f\"([{''.join(re.escape(p) for p in punctuations)}])\"\n",
    "\n",
    "    # Merge spaces around hyphen\n",
    "    merged_sentence = re.sub(r\"\\s-\\s\", \"-\", sentence)\n",
    "\n",
    "    # Merge spaces around other punctuations\n",
    "    merged_sentence = re.sub(rf\"\\s({pattern})\", r\"\\1\", merged_sentence)\n",
    "\n",
    "    return merged_sentence\n",
    "\n",
    "\n",
    "def clean_armenian_strings(strings):\n",
    "    # Armenian Unicode range: '\\u0531-\\u0587' covers Armenian capital and small letters\n",
    "    armenian_pattern = re.compile(r'[^\\u0531-\\u0587]')\n",
    "\n",
    "    cleaned_strings = []\n",
    "    for s in strings:\n",
    "        cleaned_string = armenian_pattern.sub('', s)  # Remove all non-Armenian characters\n",
    "        cleaned_strings.append(cleaned_string)\n",
    "\n",
    "    return cleaned_strings\n",
    "\n",
    "\n",
    "def add_hyphen(sentence):\n",
    "    # Avoid changing the phrases in the valid list\n",
    "    for phrase in phrases_with_i:\n",
    "        if phrase in sentence:\n",
    "            sentence = sentence.replace(phrase, phrase.replace(\" \", \"_\"))  # Temporarily replace valid phrases\n",
    "\n",
    "    # Add hyphen before standalone \"ի\" when it's not part of a valid phrase\n",
    "    updated_sentence = re.sub(r'\\b(\\w+)\\s(ի)\\b', r'\\1-\\2', sentence)\n",
    "\n",
    "    # Restore the valid phrases back to their original form\n",
    "    for phrase in phrases_with_i:\n",
    "        sentence_with_valid_phrases = phrase.replace(\" \", \"_\")\n",
    "        updated_sentence = updated_sentence.replace(sentence_with_valid_phrases, phrase)\n",
    "\n",
    "    return updated_sentence\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"բարև ձեզ խնդրում եմ փոխանցել քսանվեց հազար դրամը վահան եղոյանին։\"\n",
    "\n",
    "# Replace 'եւ' with 'և' in the input sentence\n",
    "input_sentence = replace_and(input_sentence)\n",
    "\n",
    "# Ensure the input sentence ends with a colon\n",
    "input_sentence = ensure_colon(input_sentence)\n",
    "\n",
    "corrected_sentence = add_space_between_punctuation(add_hyphen(input_sentence), allowed_punctuations)\n",
    "after_model = correct_sentence(corrected_sentence, tokenizer, model)\n",
    "merged_sentence = merge_same_punctuation(replace_and(after_model), allowed_punctuations)\n",
    "\n",
    "\n",
    "converter = ArmenianTextToNumberConverter()\n",
    "\n",
    "\n",
    "\n",
    "# Output results\n",
    "print(f\"Original Sentence: {input_sentence}\")\n",
    "print(f\"Corrected Sentence (Spaces Added): {corrected_sentence}\")\n",
    "print(f\"Model-Corrected Sentence: {after_model}\")\n",
    "print(f\"Final Merged Sentence: {merged_sentence}\")\n"
   ],
   "id": "3d51f94ff4b7cc12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: բարև ձեզ խնդրում եմ փոխանցել քսանվեց հազար դրամը վահան եղոյանին։\n",
      "Corrected Sentence (Spaces Added): բարև ձեզ խնդրում եմ փոխանցել քսանվեց հազար դրամը վահան եղոյանին ։\n",
      "Model-Corrected Sentence: Բարեւ Ձեզ խնդրում եմ փոխանցել քսանվեց հազար դրամը Վահան Եղոյանին ։\n",
      "Final Merged Sentence: Բարև Ձեզ խնդրում եմ փոխանցել քսանվեց հազար դրամը Վահան Եղոյանին։\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "2e549c9d937585b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
